{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <h1 style=\"color:darkblue\">üöÄ An√°lise de Embeddings de Tweets üìâ</h1>\n",
    "\n",
    "</div>\n",
    "\n",
    "## üìå Introdu√ß√£o\n",
    "\n",
    "\n",
    "Nesse notebook, vamos usar vetores sem√¢nticos, uma ideia que se baseia na hip√≥tese de distribui√ß√£o ao aprender a representar palavras, chamada de **embeddings**. Vamos usar essas representa√ß√µes para analisar a similaridade entre tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import math\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "nlp.component_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparo dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o conjunto de dados e selecionar as colunas de interesse\n",
    "df = pd.read_csv(\"../data/Corona_NLP_train.csv\", encoding=\"latin1\")\n",
    "df = df[[\"OriginalTweet\", \"Sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = {\n",
    "    \"covid\",\n",
    "    \"coronavirus\",\n",
    "    \"covid19\",\n",
    "    \"corona\",\n",
    "    \"coranaviru\",\n",
    "    \"covid2019\",\n",
    "    \"coronacrisis\",\n",
    "    \"coronavirusoutbreak\",\n",
    "    \"coronaviruspandemic\",\n",
    "    \"coronavirusupdate\",\n",
    "    \"coronavirusupdates\",\n",
    "    \"coronavirususa\",\n",
    "    \"coronavirusuk\",\n",
    "    \"covid19uk\",\n",
    "    \"covid19usa\",\n",
    "    \"19\",\n",
    "    \"2019\",\n",
    "    \"amp\",  # provavelmente &amp;\n",
    "    # Palavras tiradas do wordcloud presentes em todos os sentimentos\n",
    "    \"food\",\n",
    "    \"prices\",\n",
    "    \"people\",\n",
    "    \"store\",\n",
    "    \"supermarket\",\n",
    "    \"grocery\",\n",
    "    \"will\",\n",
    "}\n",
    "\n",
    "df[\"CleanTweet\"] = (\n",
    "    df[\"OriginalTweet\"]\n",
    "    .str.replace(r\"https\\S+|www\\S+|https\\S+\", \"\", regex=True)\n",
    "    .str.replace(r\"\\@\\w+\", \"\", regex=True)\n",
    "    .str.replace(r\"\\#(\\w+)\", \"\", regex=True)\n",
    "    .str.normalize(\"NFKD\")\n",
    "    .str.encode(\"ascii\", errors=\"ignore\")\n",
    "    .str.decode(\"utf-8\")\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .apply(\n",
    "        lambda text: \" \".join(\n",
    "            [\n",
    "                word\n",
    "                for word in text.split()\n",
    "                if word.lower() not in stopwords\n",
    "                and word.isalpha()\n",
    "                and len(word) > 2\n",
    "                and word.lower() not in custom_stopwords\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "df = df.loc[df[\"CleanTweet\"].str.split().str.len() > 2]\n",
    "df = df[df[\"CleanTweet\"] != \"\"]\n",
    "# df = df.drop(columns=[\"OriginalTweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An√°lise de Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Token to Vector\n",
    "\n",
    "A primeira abordagem √© a extra√ß√£o de *embeddings* de palavras. Para isso, vamos usar a classe `Tok2Vec` do pacote `spacy`[[2]](https://spacy.io/api/tok2vec). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(df[\"CleanTweet\"])\n",
    "vectors = np.array([doc.vector for doc in tqdm(docs, total=len(df))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos o algoritmo de clusteriza√ß√£o `KMeans` para agrupar os vetores sem√¢nticos. O n√∫mero de clusters √© definido pelo m√©todo do cotovelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_number_of_clusters(wcss):\n",
    "    x1, y1 = 2, wcss[0]\n",
    "    x2, y2 = 20, wcss[len(wcss) - 1]\n",
    "\n",
    "    distances = []\n",
    "    for i in range(len(wcss)):\n",
    "        x0 = i + 2\n",
    "        y0 = wcss[i]\n",
    "        numerator = abs((y2 - y1) * x0 - (x2 - x1) * y0 + x2 * y1 - y2 * x1)\n",
    "        denominator = math.sqrt((y2 - y1) ** 2 + (x2 - x1) ** 2)\n",
    "        distances.append(numerator / denominator)\n",
    "\n",
    "    return distances.index(max(distances)) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for k in tqdm(range(2, 21)):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(vectors)\n",
    "    inertia.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, 21), inertia, marker=\"o\")\n",
    "n_clusters = optimal_number_of_clusters(inertia)\n",
    "\n",
    "plt.axvline(x=n_clusters, color=\"red\", linestyle=\"--\")\n",
    "plt.text(\n",
    "    n_clusters + 0.5,\n",
    "    inertia[n_clusters - 2] + 1e6,\n",
    "    f\"n_clusters = {n_clusters}\",\n",
    "    fontsize=9,\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"N√∫mero de clusters\")\n",
    "plt.ylabel(\"In√©rcia\")\n",
    "plt.title(\"M√©todo do cotovelo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ap√≥s obter o ponto de corte, vamos usar o algoritmo `KMeans` para agrupar os vetores sem√¢nticos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(vectors)\n",
    "\n",
    "df[\"Cluster\"] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, vemos a similaridade entre os clusters usando a dist√¢ncia de cosseno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print(\"=======\" * 10)\n",
    "    print(f\"Cluster {i}:\")\n",
    "    center = centers[i]\n",
    "    top_similarities = np.argsort(-vectors.dot(center))[:20]\n",
    "    top = df.iloc[top_similarities].drop_duplicates(\"OriginalTweet\").head(10)\n",
    "    for j, row in top.iterrows():\n",
    "        print(f\"{j}:  {row['OriginalTweet']}\")\n",
    "    print()\n",
    "    print(\"=======\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar os clusters, usamos o algoritmo `t-SNE` para reduzir a dimensionalidade dos vetores sem√¢nticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "vectors_2d = tsne.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"x\"] = vectors_2d[:, 0]\n",
    "df[\"y\"] = vectors_2d[:, 1]\n",
    "\n",
    "df[\"Cluster\"] = df[\"Cluster\"].astype(str)\n",
    "\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"Cluster\",\n",
    "    hover_data=[\"CleanTweet\", \"Sentiment\"],\n",
    "    title=\"Clusters\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
