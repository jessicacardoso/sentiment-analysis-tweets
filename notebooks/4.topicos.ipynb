{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/Corona_NLP_train.csv\", encoding=\"latin1\")\n",
    "df = df[[\"OriginalTweet\", \"Sentiment\"]]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = {\n",
    "    \"covid\",\n",
    "    \"coronavirus\",\n",
    "    \"covid19\",\n",
    "    \"corona\",\n",
    "    \"coranaviru\",\n",
    "    \"covid2019\",\n",
    "    \"coronacrisis\",\n",
    "    \"coronavirusoutbreak\",\n",
    "    \"coronaviruspandemic\",\n",
    "    \"coronavirusupdate\",\n",
    "    \"coronavirusupdates\",\n",
    "    \"coronavirususa\",\n",
    "    \"coronavirusuk\",\n",
    "    \"covid19uk\",\n",
    "    \"covid19usa\",\n",
    "    \"19\",\n",
    "    \"2019\",\n",
    "    \"amp\",  # provavelmente &amp;\n",
    "    # Palavras tiradas do wordcloud presentes em todos os sentimentos\n",
    "    \"food\",\n",
    "    \"prices\",\n",
    "    \"people\",\n",
    "    \"store\",\n",
    "    \"supermarket\",\n",
    "    \"grocery\",\n",
    "    \"will\",\n",
    "}\n",
    "\n",
    "df[\"CleanedTweet\"] = (\n",
    "    df[\"OriginalTweet\"]\n",
    "    .str.replace(r\"https\\S+|www\\S+|https\\S+\", \"\", regex=True)\n",
    "    .str.replace(r\"\\@\\w+\", \"\", regex=True)\n",
    "    .str.replace(r\"\\#(\\w+)\", \"\", regex=True)\n",
    "    .str.normalize(\"NFKD\")\n",
    "    .str.encode(\"ascii\", errors=\"ignore\")\n",
    "    .str.decode(\"utf-8\")\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .apply(\n",
    "        lambda text: \" \".join(\n",
    "            [\n",
    "                word\n",
    "                for word in text.split()\n",
    "                if word.lower() not in stopwords\n",
    "                and word.isalpha()\n",
    "                and len(word) > 2\n",
    "                and word.lower() not in custom_stopwords\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "df = df.loc[df[\"CleanedTweet\"].str.split().str.len() > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(df[\"CleanedTweet\"])\n",
    "\n",
    "df[\"CleanedTweet\"] = [\n",
    "    \" \".join([token.lemma_ for token in doc if not token.is_punct]) for doc in docs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df[\"CleanedTweet\"].str.cat(sep=\" \").split()\n",
    "types = Counter(words)\n",
    "\n",
    "print(f\"Total de palavras: {len(words):,}\")\n",
    "print(f\"Tamanho do vocabulário: {len(types):,}\")\n",
    "print(f\"Riqueza do corpus: {len(types) / len(words):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_freq_words = {word for word, freq in types.items() if freq == 1}\n",
    "print(f\"Palavras de baixa frequência: {len(low_freq_words):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CleanedTweet\"] = df[\"CleanedTweet\"].apply(\n",
    "    lambda text: \" \".join([word for word in text.split() if word not in low_freq_words])\n",
    ")\n",
    "df = df[df[\"CleanedTweet\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "\n",
    "mdl = tp.LDAModel(k=10)\n",
    "\n",
    "for doc in df[\"CleanedTweet\"]:\n",
    "    mdl.add_doc(doc.split())\n",
    "\n",
    "for i in range(0, 100, 10):\n",
    "    mdl.train(10)\n",
    "    print(\"Iteration: {}\\tLog-likelihood: {}\".format(i, mdl.ll_per_word))\n",
    "\n",
    "for k in range(mdl.k):\n",
    "    print(\"Top 10 words of topic #{}\".format(k))\n",
    "    print(mdl.get_topic_words(k, top_n=10))\n",
    "\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "for k in range(mdl.k):\n",
    "    topics_k = mdl.get_topic_words(k, top_n=10)\n",
    "    topics_k = pd.DataFrame(topics_k, columns=[\"word\", \"prob\"])\n",
    "    topics_k[\"topic\"] = k + 1\n",
    "    topics.append(topics_k)\n",
    "\n",
    "topics = pd.concat(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn.objects as so\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 15), sharex=True)\n",
    "\n",
    "for k, ax in enumerate(axes.ravel()):\n",
    "    (\n",
    "        so.Plot(topics[topics[\"topic\"] == k + 1], x=\"prob\", y=\"word\")\n",
    "        .add(so.Bar())\n",
    "        .on(ax)\n",
    "        .plot()\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
