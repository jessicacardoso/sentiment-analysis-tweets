{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <h1 style=\"color:darkblue\"> Classifica√ß√£o de sentimentos nos Tweets - Parte 1üê¶</h1>\n",
    "</div>\n",
    "\n",
    "Nesse notebook, vamos abordar um problema de classifica√ß√£o de sentimentos em tweets. O objetivo √© classificar os tweets em cinco categorias: muito negativo, negativo, neutro, positivo e muito positivo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importando as bibliotecas necess√°rias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Palavras que n√£o s√£o relevantes para a an√°lise, obtidas a partir da an√°lise de frequ√™ncia\n",
    "CUSTOM_STOPWORDS = {\n",
    "    \"covid\",\n",
    "    \"coronavirus\",\n",
    "    \"corona\",\n",
    "    \"coranaviru\",\n",
    "    \"coronacrisis\",\n",
    "    \"coronavirusoutbreak\",\n",
    "    \"coronaviruspandemic\",\n",
    "    \"coronavirusupdate\",\n",
    "    \"coronavirusupdates\",\n",
    "    \"coronavirususa\",\n",
    "    \"coronavirusuk\",\n",
    "    \"coviduk\",\n",
    "    \"covidusa\",\n",
    "}\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS | CUSTOM_STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Corona_NLP_train.csv\", encoding=\"latin1\")\n",
    "df = df[[\"OriginalTweet\", \"Sentiment\"]]\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return (\n",
    "        text.str.lower()\n",
    "        # remove links\n",
    "        .str.replace(r\"https\\S+|www\\S+|https\\S+\", \"\", regex=True)\n",
    "        # remove usernames\n",
    "        .str.replace(r\"\\@\\w+\", \"\", regex=True)\n",
    "        # remove hashtags\n",
    "        .str.replace(r\"\\#(\\w+)\", \"\", regex=True)\n",
    "        # remove non-ascii characters\n",
    "        .str.normalize(\"NFKD\")\n",
    "        .str.encode(\"ascii\", errors=\"ignore\")\n",
    "        .str.decode(\"utf-8\")\n",
    "        # manter apenas letras, espa√ßos e ap√≥strofos\n",
    "        .str.replace(r\"[^a-z\\s\\']\", \"\", regex=True)\n",
    "        # remove excesso de espa√ßos\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        # remove espa√ßos no come√ßo e no fim\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "\n",
    "df[\"CleanTweet\"] = preprocess_text(df[\"OriginalTweet\"])\n",
    "df[\"CleanTweetNoStopwords\"] = df[\"CleanTweet\"].apply(\n",
    "    lambda text: \" \".join([word for word in text.split() if word not in STOPWORDS])\n",
    ")\n",
    "df = df.loc[df[\"CleanTweet\"].str.split().str.len() > 2]\n",
    "df = df.drop_duplicates(subset=[\"CleanTweet\", \"Sentiment\"])\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No notebook de explora√ß√£o de dados, vimos que mais da metade do vocabul√°rio dos tweets era composto por palavras de frequ√™ncia √∫nica. Para acelerar o processamento, vamos remover essas palavras do vocabul√°rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df[\"CleanTweet\"].str.cat(sep=\" \").split()\n",
    "types = Counter(words)\n",
    "hapax = set([word for word, count in types.items() if count <= 1])\n",
    "\n",
    "print(f\"Total de palavras: {len(words):,}\")\n",
    "print(f\"Tamanho do vocabul√°rio: {len(types):,}\")\n",
    "print(f\"Palavras √∫nicas: {len(hapax):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CleanTweet\"] = df[\"CleanTweet\"].apply(\n",
    "    lambda text: \" \".join([word for word in text.split() if word not in hapax])\n",
    ")\n",
    "# remove tweets com menos de 3 palavras\n",
    "df = df.loc[df[\"CleanTweet\"].str.split().str.len() > 2]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(df[\"CleanTweet\"])\n",
    "\n",
    "df[\"Lemmatized\"] = [\n",
    "    \" \".join([token.lemma_ for token in doc])\n",
    "    for doc in tqdm(docs, total=len(df), desc=\"Lemmatizing\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(df[\"Lemmatized\"])\n",
    "df[\"LemmatizedNoStopwords\"] = [\n",
    "    \" \".join([token.text for token in doc if token.text not in STOPWORDS])\n",
    "    for doc in tqdm(docs, total=len(df), desc=\"Extracting stopwords\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Treinamento\n",
    "\n",
    "No pr√©-processamento, criamos quatro colunas no dataframe de treino: \n",
    "- `CleanTweet`: tweets padronizados em min√∫sculas e sem caracteres especiais\n",
    "- `CleanTweetNoStopwords`: tweets padronizados sem stopwords\n",
    "- `Lemmatized`: tweets padronizados e lematizados\n",
    "- `LemmatizedNoStopwords`: tweets padronizados, lematizados e sem stopwords\n",
    "\n",
    "Pretendemos treinar um modelo para cada coluna a fim de comparar a performance dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sentiment\"] = pd.Categorical(\n",
    "    df[\"Sentiment\"],\n",
    "    categories=[\n",
    "        \"Extremely Negative\",\n",
    "        \"Negative\",\n",
    "        \"Neutral\",\n",
    "        \"Positive\",\n",
    "        \"Extremely Positive\",\n",
    "    ],\n",
    "    ordered=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    confusion_matrix, target_names, title=\"Confusion matrix\", cmap=None, normalize=True\n",
    "):\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap(\"Blues\")\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(confusion_matrix, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = range(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        confusion_matrix = (\n",
    "            confusion_matrix.astype(\"float\")\n",
    "            / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "        )\n",
    "\n",
    "    thresh = confusion_matrix.max() / 1.5 if normalize else confusion_matrix.max() / 2\n",
    "    for i, j in itertools.product(\n",
    "        range(confusion_matrix.shape[0]), range(confusion_matrix.shape[1])\n",
    "    ):\n",
    "        if normalize:\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                \"{:0.4f}\".format(confusion_matrix[i, j]),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if confusion_matrix[i, j] > thresh else \"black\",\n",
    "            )\n",
    "        else:\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                \"{:,}\".format(confusion_matrix[i, j]),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if confusion_matrix[i, j] > thresh else \"black\",\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"MultinomialNB\",\n",
    "    \"LogisticRegression\",\n",
    "    \"RandomForestClassifier\",\n",
    "    \"LinearSVC\",\n",
    "}\n",
    "df = df.reset_index(drop=True)\n",
    "X = df[[\"CleanTweet\", \"CleanTweetNoStopwords\", \"Lemmatized\", \"LemmatizedNoStopwords\"]]\n",
    "y = df[\"Sentiment\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Bag of Words vs TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_count_vectorizer = CountVectorizer()\n",
    "\n",
    "clean_no_stopwords_count_vectorizer = CountVectorizer()\n",
    "\n",
    "lemmatized_count_vectorizer = CountVectorizer()\n",
    "\n",
    "lemmatized_no_stopwords_count_vectorizer = CountVectorizer()\n",
    "\n",
    "clean_tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "clean_no_stopwords_tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "lemmatized_tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "lemmatized_no_stopwords_tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectorizers = {\n",
    "    \"CountVectorizer\": {\n",
    "        \"CleanTweet\": clean_count_vectorizer.fit(X_train[\"CleanTweet\"]),\n",
    "        \"CleanTweetNoStopwords\": clean_no_stopwords_count_vectorizer.fit(\n",
    "            X_train[\"CleanTweetNoStopwords\"]\n",
    "        ),\n",
    "        \"Lemmatized\": lemmatized_count_vectorizer.fit(X_train[\"Lemmatized\"]),\n",
    "        \"LemmatizedNoStopwords\": lemmatized_no_stopwords_count_vectorizer.fit(\n",
    "            X_train[\"LemmatizedNoStopwords\"]\n",
    "        ),\n",
    "    },\n",
    "    \"TfidfVectorizer\": {\n",
    "        \"CleanTweet\": clean_tfidf_vectorizer.fit(X_train[\"CleanTweet\"]),\n",
    "        \"CleanTweetNoStopwords\": clean_no_stopwords_tfidf_vectorizer.fit(\n",
    "            X_train[\"CleanTweetNoStopwords\"]\n",
    "        ),\n",
    "        \"Lemmatized\": lemmatized_tfidf_vectorizer.fit(X_train[\"Lemmatized\"]),\n",
    "        \"LemmatizedNoStopwords\": lemmatized_no_stopwords_tfidf_vectorizer.fit(\n",
    "            X_train[\"LemmatizedNoStopwords\"]\n",
    "        ),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CountVectorizer()\n",
    "X_train_vec = clf.fit_transform(X_train[\"CleanTweet\"])\n",
    "X_test_vec = clf.transform(X_test[\"CleanTweet\"])\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(fitted_models, fitted_vectorizers, X_test, y_test):\n",
    "    results = {}\n",
    "    for model_name, model in tqdm(fitted_models.items(), desc=\"Evaluating models\"):\n",
    "        results[model_name] = {}\n",
    "        for column_name, vectorizer in tqdm(\n",
    "            fitted_vectorizers.items(), desc=f\"Evaluating {model_name}\", leave=False\n",
    "        ):\n",
    "            X_test_vectorized = vectorizer.transform(X_test[column_name])\n",
    "            y_pred = model[column_name].predict(X_test_vectorized)\n",
    "            results[model_name][column_name] = {\n",
    "                \"classification_report\": classification_report(\n",
    "                    y_test, y_pred, target_names=y.cat.categories\n",
    "                ),\n",
    "                \"confusion_matrix\": confusion_matrix(y_test, y_pred, normalize=\"true\"),\n",
    "            }\n",
    "            print(f\"Evaluating {model_name} for {column_name}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def fit_models(models, fitted_vectorizers, y_train):\n",
    "    fitted_models = {}\n",
    "    for model_name in tqdm(models, desc=\"Fitting models\"):\n",
    "        fitted_models[model_name] = {}\n",
    "        for column_name, vectorizer in fitted_vectorizers.items():\n",
    "            print(f\"Fitting {model_name} for {column_name}\")\n",
    "            X_train_vectorized = vectorizer.transform(X_train[column_name])\n",
    "            if model_name == \"MultinomialNB\":\n",
    "                model = MultinomialNB()\n",
    "            elif model_name == \"LogisticRegression\":\n",
    "                model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "            elif model_name == \"RandomForestClassifier\":\n",
    "                model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "            elif model_name == \"LinearSVC\":\n",
    "                model = LinearSVC(dual=\"auto\", random_state=42)\n",
    "            fitted_model = model.fit(X_train_vectorized, y_train)\n",
    "            fitted_models[model_name][column_name] = fitted_model\n",
    "        print()\n",
    "    return fitted_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_models = fit_models(models, vectorizers[\"CountVectorizer\"], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_models(fitted_models, vectorizers[\"CountVectorizer\"], X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_results in results.items():\n",
    "    for column_name, column_results in model_results.items():\n",
    "        print(f\"{model_name} - {column_name}\")\n",
    "        print(column_results[\"classification_report\"])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_results in results.items():\n",
    "    for column_name, column_results in model_results.items():\n",
    "        plot_confusion_matrix(\n",
    "            column_results[\"confusion_matrix\"],\n",
    "            y.cat.categories,\n",
    "            title=f\"{model_name} - {column_name}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_models_tfidf = fit_models(models, vectorizers[\"TfidfVectorizer\"], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tfidf = evaluate_models(\n",
    "    fitted_models_tfidf, vectorizers[\"TfidfVectorizer\"], X_test, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_results in results_tfidf.items():\n",
    "    for column_name, column_results in model_results.items():\n",
    "        print(f\"{model_name} - {column_name}\")\n",
    "        print(column_results[\"classification_report\"])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_results in results_tfidf.items():\n",
    "    for column_name, column_results in model_results.items():\n",
    "        plot_confusion_matrix(\n",
    "            column_results[\"confusion_matrix\"],\n",
    "            y.cat.categories,\n",
    "            title=f\"{model_name} - {column_name}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets_vec = np.array(\n",
    "    [\n",
    "        doc.vector\n",
    "        for doc in tqdm(\n",
    "            nlp.pipe(df[\"CleanTweet\"]), total=len(df), desc=\"Vectorizing tweets\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "clean_no_stopwords_vec = np.array(\n",
    "    [\n",
    "        doc.vector\n",
    "        for doc in tqdm(\n",
    "            nlp.pipe(df[\"CleanTweetNoStopwords\"]),\n",
    "            total=len(df),\n",
    "            desc=\"Vectorizing tweets\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "lemmatized_vec = np.array(\n",
    "    [\n",
    "        doc.vector\n",
    "        for doc in tqdm(\n",
    "            nlp.pipe(df[\"Lemmatized\"]), total=len(df), desc=\"Vectorizing tweets\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "lemmatized_no_stopwords_vec = np.array(\n",
    "    [\n",
    "        doc.vector\n",
    "        for doc in tqdm(\n",
    "            nlp.pipe(df[\"LemmatizedNoStopwords\"]),\n",
    "            total=len(df),\n",
    "            desc=\"Vectorizing tweets\",\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_models = {}\n",
    "\n",
    "for model_name in [\"LogisticRegression\", \"RandomForestClassifier\", \"LinearSVC\"]:\n",
    "    fitted_models[model_name] = {}\n",
    "    for column_name, vec in zip(\n",
    "        [\n",
    "            \"CleanTweet\",\n",
    "            \"CleanTweetNoStopwords\",\n",
    "            \"Lemmatized\",\n",
    "            \"LemmatizedNoStopwords\",\n",
    "        ],\n",
    "        [\n",
    "            clean_tweets_vec,\n",
    "            clean_no_stopwords_vec,\n",
    "            lemmatized_vec,\n",
    "            lemmatized_no_stopwords_vec,\n",
    "        ],\n",
    "    ):\n",
    "        print(f\"Fitting {model_name} for {column_name}\")\n",
    "        if model_name == \"LogisticRegression\":\n",
    "            model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "        elif model_name == \"RandomForestClassifier\":\n",
    "            model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "        elif model_name == \"LinearSVC\":\n",
    "            model = LinearSVC(dual=\"auto\", random_state=42)\n",
    "\n",
    "        X_train_vec = vec[y_train.index]\n",
    "        fitted_model = model.fit(X_train_vec, y_train)\n",
    "\n",
    "        X_test_vec = vec[y_test.index]\n",
    "        y_pred = fitted_model.predict(X_test_vec)\n",
    "\n",
    "        report = classification_report(y_test, y_pred, target_names=y.cat.categories)\n",
    "        cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "\n",
    "        fitted_models[model_name][column_name] = {\n",
    "            \"model\": fitted_model,\n",
    "            \"classification_report\": report,\n",
    "            \"confusion_matrix\": cm,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_results in fitted_models.items():\n",
    "    for column_name, column_results in model_results.items():\n",
    "        print(f\"{model_name} - {column_name}\")\n",
    "        print(column_results[\"classification_report\"])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_results in fitted_models.items():\n",
    "    for column_name, column_results in model_results.items():\n",
    "        plot_confusion_matrix(\n",
    "            column_results[\"confusion_matrix\"],\n",
    "            y.cat.categories,\n",
    "            title=f\"{model_name} - {column_name}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sentences Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_embeddings = model.encode(df[\"CleanTweet\"].tolist(), show_progress_bar=True)\n",
    "clean_no_stops = model.encode(\n",
    "    df[\"CleanTweetNoStopwords\"].tolist(), show_progress_bar=True\n",
    ")\n",
    "lemmatized_embeddings = model.encode(df[\"Lemmatized\"].tolist(), show_progress_bar=True)\n",
    "lemmatized_no_stops = model.encode(\n",
    "    df[\"LemmatizedNoStopwords\"].tolist(), show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_models = {}\n",
    "\n",
    "for model_name in [\"LogisticRegression\", \"RandomForestClassifier\", \"LinearSVC\"]:\n",
    "    fitted_models[model_name] = {}\n",
    "    for column_name, vec in zip(\n",
    "        [\n",
    "            \"CleanTweet\",\n",
    "            \"CleanTweetNoStopwords\",\n",
    "            \"Lemmatized\",\n",
    "            \"LemmatizedNoStopwords\",\n",
    "        ],\n",
    "        [\n",
    "            clean_embeddings,\n",
    "            clean_no_stops,\n",
    "            lemmatized_embeddings,\n",
    "            lemmatized_no_stops,\n",
    "        ],\n",
    "    ):\n",
    "        print(f\"Fitting {model_name} for {column_name}\")\n",
    "        if model_name == \"LogisticRegression\":\n",
    "            model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "        elif model_name == \"RandomForestClassifier\":\n",
    "            model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "        elif model_name == \"LinearSVC\":\n",
    "            model = LinearSVC(dual=\"auto\", random_state=42)\n",
    "\n",
    "        X_train_vec = vec[y_train.index]\n",
    "        fitted_model = model.fit(X_train_vec, y_train)\n",
    "\n",
    "        X_test_vec = vec[y_test.index]\n",
    "        y_pred = fitted_model.predict(X_test_vec)\n",
    "\n",
    "        report = classification_report(y_test, y_pred, target_names=y.cat.categories)\n",
    "        cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "\n",
    "        fitted_models[model_name][column_name] = {\n",
    "            \"model\": fitted_model,\n",
    "            \"classification_report\": report,\n",
    "            \"confusion_matrix\": cm,\n",
    "        }\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_results in fitted_models.items():\n",
    "    for column_name, column_results in model_results.items():\n",
    "        print(f\"{model_name} - {column_name}\")\n",
    "        print(column_results[\"classification_report\"])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_results in fitted_models.items():\n",
    "    for column_name, column_results in model_results.items():\n",
    "        plot_confusion_matrix(\n",
    "            column_results[\"confusion_matrix\"],\n",
    "            y.cat.categories,\n",
    "            title=f\"{model_name} - {column_name}\",\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
