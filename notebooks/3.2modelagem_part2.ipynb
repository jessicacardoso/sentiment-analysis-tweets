{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <h1 style=\"color:darkblue\"> Classifica√ß√£o de sentimentos nos Tweets - Parte 2üê¶</h1>\n",
    "</div>\n",
    "\n",
    "Nesse notebook, vamos continuar a an√°lise dos tweets, mas agora vamos prev√™-los para tr√™s classes: positivo, negativo e neutro. No notebook anterior, fizemos a an√°lise explorat√≥ria dos tweets e a classifica√ß√£o em cinco classes. Vamos usar a limpeza do notebook anterior e comparar com a vers√£o lematizada dos tweets. \n",
    "\n",
    "Ser√£o treinados dois modelos de classifica√ß√£o: um com os tweets limpos e outro com os tweets lematizados. Ao final, vamos comparar os resultados e verificar se a lematiza√ß√£o dos tweets tem impacto na performance do modelo.\n",
    "\n",
    "Os modelos de classifica√ß√£o que vamos usar s√£o os mesmos do notebook anterior:\n",
    "- Regress√£o Log√≠stica\n",
    "- Naive Bayes\n",
    "- Floresta Aleat√≥ria\n",
    "- SVM Linear\n",
    "\n",
    "Al√©m disso, vamos usar a t√©cnica de vetoriza√ß√£o dos textos com o TF-IDF. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Corona_NLP_train.csv\", encoding=\"latin1\")\n",
    "df = df[[\"OriginalTweet\", \"Sentiment\"]]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sentiment\"] = df[\"Sentiment\"].replace(\n",
    "    {\"Extremely Negative\": \"Negative\", \"Extremely Positive\": \"Positive\"}\n",
    ")\n",
    "df[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepara√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return (\n",
    "        text.str.lower()\n",
    "        # remove links\n",
    "        .str.replace(r\"https\\S+|www\\S+|https\\S+\", \"\", regex=True)\n",
    "        # remove usernames\n",
    "        .str.replace(r\"\\@\\w+\", \"\", regex=True)\n",
    "        # remove hashtags\n",
    "        .str.replace(r\"\\#(\\w+)\", \"\", regex=True)\n",
    "        # remove non-ascii characters\n",
    "        .str.normalize(\"NFKD\")\n",
    "        .str.encode(\"ascii\", errors=\"ignore\")\n",
    "        .str.decode(\"utf-8\")\n",
    "        # manter apenas letras, espa√ßos e ap√≥strofos\n",
    "        .str.replace(r\"[^a-z\\s\\']\", \"\", regex=True)\n",
    "        # remove excesso de espa√ßos\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        # remove espa√ßos no come√ßo e no fim\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "\n",
    "df[\"CleanTweet\"] = preprocess_text(df[\"OriginalTweet\"])\n",
    "\n",
    "# Remover palavras que aparecem apenas uma vez\n",
    "words = df[\"CleanTweet\"].str.cat(sep=\" \").split()\n",
    "types = Counter(words)\n",
    "hapax = set([word for word, count in types.items() if count <= 1])\n",
    "\n",
    "df[\"CleanTweet\"] = df[\"CleanTweet\"].apply(\n",
    "    lambda text: \" \".join([word for word in text.split() if word not in hapax])\n",
    ")\n",
    "\n",
    "# Manter apenas tweets com mais de 2 palavras\n",
    "df = df.loc[df[\"CleanTweet\"].str.split().str.len() > 2]\n",
    "df = df.drop_duplicates(subset=[\"CleanTweet\", \"Sentiment\"])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(df[\"CleanTweet\"])\n",
    "\n",
    "df[\"Lemmatized\"] = [\n",
    "    \" \".join([token.lemma_ for token in doc])\n",
    "    for doc in tqdm(docs, total=len(df), desc=\"Lemmatizing\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\n",
    "    df[\"Lemmatized\"].duplicated(keep=False),\n",
    "    [\"OriginalTweet\", \"CleanTweet\", \"Sentiment\"],\n",
    "].sort_values(\"CleanTweet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao aplicar a lematiza√ß√£o notamos *tweets* duplicados, por isso, vamos remover esses *tweets* duplicados considerando o texto lematizado e o sentimento. Ao executar a c√©lula acima para obter os *tweets* duplicados, vemos tamb√©m uma inconsist√™ncia no *dataset* , onde um texto similar tem sentimentos diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[15757, \"OriginalTweet\"], df.loc[21677, \"OriginalTweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=[\"Lemmatized\", \"Sentiment\"])\n",
    "df = df.drop_duplicates(subset=[\"Lemmatized\"], keep=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Classifica√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    \"LinearSVC\": LinearSVC(dual=\"auto\", random_state=42),\n",
    "}\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Sentiment\"])\n",
    "y = pd.Categorical(\n",
    "    df[\"Sentiment\"], categories=[\"Negative\", \"Neutral\", \"Positive\"], ordered=True\n",
    ")\n",
    "y = pd.Series(y, name=\"Sentiment\", index=X.index)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.05, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_parquet(\"../data/X_train.parquet\", index=False)\n",
    "# X_test.to_parquet(\"../data/X_test.parquet\", index=False)\n",
    "# y_train.to_frame().to_parquet(\"../data/y_train.parquet\", index=False)\n",
    "# y_test.to_frame().to_parquet(\"../data/y_test.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 1: Tweets Limpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X_train_vectorized = clean_tweet_vectorizer.fit_transform(X_train[\"CleanTweet\"])\n",
    "X_test_vectorized = clean_tweet_vectorizer.transform(X_test[\"CleanTweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "print(\"LIMPEZA SEM LEMA:\")\n",
    "for model_name, model in tqdm(models.items(), desc=\"Training models\"):\n",
    "    scores = cross_validate(\n",
    "        model,\n",
    "        X_train_vectorized,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring={\n",
    "            \"accuracy\": make_scorer(accuracy_score),\n",
    "            \"precision\": make_scorer(precision_score, average=\"weighted\"),\n",
    "            \"recall\": make_scorer(recall_score, average=\"weighted\"),\n",
    "            \"f1\": make_scorer(f1_score, average=\"weighted\"),\n",
    "        },\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    results[model_name] = scores\n",
    "\n",
    "    print(f\"{model_name:=^55}\")\n",
    "    print(\n",
    "        f\"{'subset':10}\",\n",
    "        f\"{'accuracy':>10}\",\n",
    "        f\"{'precision':>10}\",\n",
    "        f\"{'recall':>10}\",\n",
    "        f\"{'f1':>10}\",\n",
    "    )\n",
    "    print(\n",
    "        f\"{'train':10}\",\n",
    "        f\"{scores['train_accuracy'].mean():10.2f}\",\n",
    "        f\"{scores['train_precision'].mean():10.2f}\",\n",
    "        f\"{scores['train_recall'].mean():10.2f}\",\n",
    "        f\"{scores['train_f1'].mean():10.2f}\",\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"{'test':10}\",\n",
    "        f\"{scores['test_accuracy'].mean():10.2f}\",\n",
    "        f\"{scores['test_precision'].mean():10.2f}\",\n",
    "        f\"{scores['test_recall'].mean():10.2f}\",\n",
    "        f\"{scores['test_f1'].mean():10.2f}\",\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, validamos os modelos nos dados de teste e comparamos os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "    y_pred = model.predict(X_test_vectorized)\n",
    "    print(f\"{model_name:=^55}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os modelos SVM Linear e Regress√£o Log√≠stica tiveram os melhores resultados. Sendo o primeiro com F1 m√©dio ponderado de 0.82 e o segundo com 0.81."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 2: Tweets limpos e lematizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "X_train_vectorized = lemmatized_vectorizer.fit_transform(X_train[\"Lemmatized\"])\n",
    "X_test_vectorized = lemmatized_vectorizer.transform(X_test[\"Lemmatized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "print(\"LIMPEZA COM LEMA:\")\n",
    "for model_name, model in tqdm(models.items(), desc=\"Training models\"):\n",
    "    scores = cross_validate(\n",
    "        model,\n",
    "        X_train_vectorized,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring={\n",
    "            \"accuracy\": make_scorer(accuracy_score),\n",
    "            \"precision\": make_scorer(precision_score, average=\"weighted\"),\n",
    "            \"recall\": make_scorer(recall_score, average=\"weighted\"),\n",
    "            \"f1\": make_scorer(f1_score, average=\"weighted\"),\n",
    "        },\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    results[model_name] = scores\n",
    "\n",
    "    print(f\"{model_name:=^55}\")\n",
    "    print(\n",
    "        f\"{'subset':10}\",\n",
    "        f\"{'accuracy':>10}\",\n",
    "        f\"{'precision':>10}\",\n",
    "        f\"{'recall':>10}\",\n",
    "        f\"{'f1':>10}\",\n",
    "    )\n",
    "    print(\n",
    "        f\"{'train':10}\",\n",
    "        f\"{scores['train_accuracy'].mean():10.2f}\",\n",
    "        f\"{scores['train_precision'].mean():10.2f}\",\n",
    "        f\"{scores['train_recall'].mean():10.2f}\",\n",
    "        f\"{scores['train_f1'].mean():10.2f}\",\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"{'test':10}\",\n",
    "        f\"{scores['test_accuracy'].mean():10.2f}\",\n",
    "        f\"{scores['test_precision'].mean():10.2f}\",\n",
    "        f\"{scores['test_recall'].mean():10.2f}\",\n",
    "        f\"{scores['test_f1'].mean():10.2f}\",\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "    y_pred = model.predict(X_test_vectorized)\n",
    "    print(f\"{model_name:=^55}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os valores de F1 m√©dio ponderado continuaram sendo os melhores para os modelos SVM Linear e Regress√£o Log√≠stica, sendo 0.81 e 0.79 respectivamente. Podemos observar que classificar os *tweets* em tr√™s classes √© mais f√°cil do que em cinco classes, pois os modelos tiveram um desempenho melhor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicional: An√°lise de Sentimentos com Mixtral: Zero-Shot Prompting\n",
    "\n",
    "Por fim, vamos usar o modelo Mixtral-8x7B-Instruct-v0.1 para classificar os *tweets* e comparar os resultados com os modelos treinados.\n",
    "\n",
    "Vamos come√ßar, preparando o ambiente de trabalho, clonando o reposit√≥rio do Mixtral e instalando as depend√™ncias necess√°rias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/dvmazur/mixtral-offloading.git --quiet\n",
    "cd mixtral-offloading && pip install -r requirements.txt --quiet\n",
    "\n",
    "# Baixar o modelo\n",
    "huggingface-cli download lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo --quiet --local-dir Mixtral-8x7B-Instruct-v0.1-offloading-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguindo o notebook de refer√™ncia do reposit√≥rio, definimos as configura√ß√µes necess√°rias para a execu√ß√£o do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"mixtral-offloading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from hqq.core.quantize import BaseQuantizeConfig\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from src.build_model import OffloadConfig, QuantConfig, build_model\n",
    "\n",
    "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
    "state_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(quantized_model_name)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "##### Change this to 5 if you have only 12 GB of GPU VRAM #####\n",
    "offload_per_layer = 1\n",
    "# offload_per_layer = 5\n",
    "###############################################################\n",
    "\n",
    "offload_config = OffloadConfig(\n",
    "    main_size=config.num_hidden_layers * (config.num_local_experts - offload_per_layer),\n",
    "    offload_size=config.num_hidden_layers * offload_per_layer,\n",
    "    buffer_size=4,\n",
    "    offload_per_layer=offload_per_layer,\n",
    ")\n",
    "\n",
    "attn_config = BaseQuantizeConfig(\n",
    "    nbits=4,\n",
    "    group_size=64,\n",
    "    quant_zero=True,\n",
    "    quant_scale=True,\n",
    ")\n",
    "attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n",
    "\n",
    "ffn_config = BaseQuantizeConfig(\n",
    "    nbits=2,\n",
    "    group_size=16,\n",
    "    quant_zero=True,\n",
    "    quant_scale=True,\n",
    ")\n",
    "quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n",
    "\n",
    "model = build_model(\n",
    "    device=device,\n",
    "    quant_config=quant_config,\n",
    "    offload_config=offload_config,\n",
    "    state_path=state_path,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos a fun√ß√£o `generate_response` para obter a classifica√ß√£o dos *tweets*. A fun√ß√£o recebe como par√¢metro o *prompt* com o texto do *tweet* e retorna a classifica√ß√£o do *tweet*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "    prompt: str, model: torch.nn.Module, tokenizer: AutoTokenizer, device: torch.device\n",
    ") -> str:\n",
    "    user_entry = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(user_entry, return_tensors=\"pt\").to(\n",
    "        device\n",
    "    )\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    result = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        past_key_values=None,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=8,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        return_dict_in_generate=True,\n",
    "        output_hidden_states=True,\n",
    "    )\n",
    "\n",
    "    sequence = result.get(\"sequences\", None)\n",
    "    if sequence is None:\n",
    "        raise ValueError(\"Generation failed\")\n",
    "\n",
    "    outputs = tokenizer.decode(sequence[0], skip_special_tokens=True)\n",
    "    return outputs.split(\"[/INST]\")[-1].strip().split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "<|system|>\n",
    "You are a tweet categorizer that only responds to whether the tweet is \"Negative\", \"Neutral\" or \"Positive\". \n",
    "You should only respond with the label in which the Tweet falls and nothing else. \n",
    "<|user|>\n",
    "Classify the text into one of these categories based on the sentiment of the tweet.\n",
    "Text: {text}\n",
    "Sentiment:\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processo de classifica√ß√£o dos *tweets* √© feito em um loop, onde cada *tweet* √© classificado e o resultado √© armazenado em uma lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    idx = random.randint(0, len(X_test))\n",
    "\n",
    "    prompt = user_prompt.format(text=X_test.iloc[idx][\"OriginalTweet\"])\n",
    "    print(\"Text:\", X_test.iloc[idx][\"OriginalTweet\"])\n",
    "    print(f\"True sentiment: {y_test.iloc[idx]}\")\n",
    "\n",
    "    response = generate_response(prompt, model, tokenizer, device)\n",
    "    print(f\"Predicted sentiment: {response}\")\n",
    "    print()\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mixtral = []\n",
    "\n",
    "for text in tqdm(X_test[\"OriginalTweet\"], desc=\"Mixtral\"):\n",
    "    response = generate_response(\n",
    "        user_prompt.format(text=text), model, tokenizer, device\n",
    "    )\n",
    "    y_pred_mixtral.append(response)\n",
    "\n",
    "y_pred_mixtral = pd.Series(y_pred_mixtral, name=\"Sentiment\", index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se observar que o modelo Mixtral-8x7B-Instruct-v0.1 obteve um desempenho inferior aos modelos treinados, com um F1 m√©dio ponderado de 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_mixtral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicional: An√°lise de Sentimentos com Bert: Fine-Tuning\n",
    "\n",
    "Nessa etapa, vamos treinar um modelo de classifica√ß√£o de sentimentos com o Bert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# X_train = pd.read_parquet(\"../data/X_train.parquet\")\n",
    "# X_test = pd.read_parquet(\"../data/X_test.parquet\")\n",
    "# y_train = pd.read_parquet(\"../data/y_train.parquet\").squeeze()\n",
    "# y_test = pd.read_parquet(\"../data/y_test.parquet\").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-cased\", num_labels=3\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return clf_metrics.compute(\n",
    "        predictions=predictions, references=labels, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "\n",
    "clf_metrics = evaluate.combine([\"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict(\n",
    "    pd.DataFrame(\n",
    "        {\"text\": X_train[\"OriginalTweet\"], \"label\": y_train.values.codes}\n",
    "    ).to_dict(orient=\"list\")\n",
    ")\n",
    "\n",
    "test_dataset = Dataset.from_dict(\n",
    "    pd.DataFrame(\n",
    "        {\"text\": X_test[\"OriginalTweet\"], \"label\": y_test.values.codes}\n",
    "    ).to_dict(orient=\"list\")\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.shuffle(seed=42).train_test_split(test_size=0.05)\n",
    "\n",
    "train_tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized_datasets = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    per_device_train_batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_datasets[\"train\"],\n",
    "    eval_dataset=train_tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando o *fine-tuning* do modelo Bert, conseguimos m√©tricas acima dos modelos anteriores, com um F1 m√©dio ponderado de 0.91 no conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_tokenized_datasets)\n",
    "\n",
    "y_pred_bert = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test.values.codes, y_pred_bert, target_names=y_test.cat.categories\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
